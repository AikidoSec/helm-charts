agent:
  image:
    repository: public.ecr.aws/aikido-cloud/kubernetes-agent
    pullPolicy: IfNotPresent
    tag: v1.11.0
  service:
    port: 8091
    metricsPort: 8080
    threatDetectionPort: 8241
  priorityClassName: ''
  controllerCacheSyncTimeout: '60m'
  externalSecret: ''
  # Enable auto-updates of the agent and SBOM collector
  # When enabled, the agent can update itself and the SBOM collector by patching deployments/daemonsets
  # When disabled, RBAC permissions for updating deployments/daemonsets will not be granted
  autoUpdate:
    enabled: true
  resources:
    requests:
      cpu: 100m
      memory: 100Mi
    limits:
      # cpu: 1500m # No CPU limit by default
      memory: 5Gi # Only Mi and Gi are supported

sbomCollector:
  # Enable or disable the SBOM collector, and all its components
  enabled: false
  image:
    repository: public.ecr.aws/aikido-cloud/kubernetes-sbom-collector
    pullPolicy: IfNotPresent
    tag: v1.5.1
  # If set to true, the agent will run the SBOM collector as a daemonSet.
  # Otherwise, it will run it as a deployment.
  runAsDaemonSet: true
  priorityClassName: ''
  resources:
    requests:
      cpu: 100m
      memory: 100Mi
      ephemeral-storage: 500Mi
    limits:
      # cpu: 1500m # No CPU limit by default
      memory: 5Gi # Only Mi and Gi are supported
      ephemeral-storage: 10Gi
  daemonSetTolerations:
    # these tolerations are to have the daemonset runnable on control plane nodes
    # remove them if your control plane nodes should not run pods
    - key: node-role.kubernetes.io/control-plane
      operator: Exists
      effect: NoSchedule
    - key: node-role.kubernetes.io/master
      operator: Exists
      effect: NoSchedule
  securityContext:
    # Running as root allows the collector to access images cached on the node
    # instead of always pulling them from the registry.
    # If your security policy does not allow this, you can adjust these values.
    runAsUser: 0
    runAsGroup: 0
    allowPrivilegeEscalation: false
    readOnlyRootFilesystem: true
    capabilities:
      drop:
        - ALL

  # Access to image pull secrets for SBOM generation
  # The SBOM collector needs to read container images to generate SBOMs.
  # When images are pulled from private registries, the collector requires
  # access to the same image pull secrets that pods use.
  secretsAccess:
    # Enable access to image pull secrets
    enabled: true
    # List of specific image pull secret names to grant access to.
    # If empty, the collector will have access to all secrets in the cluster.
    # Recommended: specify only the secrets actually used by pods' imagePullSecrets
    # to follow the principle of least privilege.
    imagePullSecretNames: []

  # Pod annotations for the SBOM collector
  # Useful for scenarios like Azure Workload Identity where specific annotations
  # are required to access container registries (e.g., ACR)
  podAnnotations: {}

  # Pod labels for the SBOM collector
  podLabels: {}

  # Service account configuration for the SBOM collector
  # If not specified, will use the root serviceAccount configuration
  serviceAccount:
    # Specifies whether a service account should be created
    create: true
    # Annotations to add to the service account
    annotations: {}
    # The name of the service account to use.
    # If not set and create is true, a name is generated using the fullname template
    name: ''
    # Image pull secrets to attach to the SBOM collector's service account.
    # These secrets are used to authenticate with private container registries when pulling images from remote registries.
    # Note: This does not grant the SBOM collector access to secrets.
    # Configure sbomCollector.secretsAccess.imagePullSecretNames for that.
    imagePullSecrets: []

  # Pull secret mount configuration
  # Mount a Docker config secret as a volume to authenticate with private registries.
  # This is useful for authenticating with registries like OpenShift's internal registry,
  # Red Hat registries, and other private registries.
  # To enable, provide a secretName. Leave empty to disable.
  pullSecretMount:
    # Name of the secret containing the Docker config (.dockerconfigjson key)
    # The secret must be of type kubernetes.io/dockerconfigjson or contain a .dockerconfigjson key
    secretName: ''
  logging:
    # Suppress error logs from the SBOM collector.
    # When true, error messages during SBOM generation, image scanning, and registry
    # authentication will not be logged.
    suppressErrors: false

config:
  apiToken: ''
  apiEndpoint: 'https://k8s.aikido-security.com'
  # HTTP proxy configuration
  proxy:
    # HTTP proxy URL (e.g., http://proxy.company.com:8080)
    httpProxy: ''
    # HTTPS proxy URL (e.g., http://proxy.company.com:8080)
    httpsProxy: ''
    # Comma-separated list of hosts to bypass proxy (e.g., localhost,127.0.0.1,.local)
    noProxy: ''

imagePullSecrets: []
nameOverride: ''
fullnameOverride: ''

networkPolicy:
  enabled: true

serviceAccount:
  # Specifies whether a service account should be created
  create: true
  # Annotations to add to the service account
  annotations: {}
  # The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  name: ''

podAnnotations: {}
podLabels: {}

podSecurityContext: {}
# fsGroup: 2000

securityContext:
  capabilities:
    drop:
      - ALL
  readOnlyRootFilesystem: true
  runAsNonRoot: true
  runAsUser: 1000
  allowPrivilegeEscalation: false

healthCheck:
  port: 8081

nodeSelector: {}

affinity: {}

tolerations: []

additionalEnvVars: []

additionalVolumes: []

additionalVolumeMounts: []

# Extra Kubernetes objects to deploy alongside the chart
# Useful for deploying ExternalSecret resources when using secret managers
# like HashiCorp Vault, AWS Secrets Manager, etc. with the ExternalSecrets Operator.
#
# Example - ExternalSecret that creates the agent's config secret:
# extraObjects:
#   - |
#     apiVersion: external-secrets.io/v1beta1
#     kind: ExternalSecret
#     metadata:
#       name: aikido-credentials
#       namespace: {{ .Release.Namespace }}
#     spec:
#       secretStoreRef:
#         kind: ClusterSecretStore
#         name: vault
#       target:
#         name: aikido-credentials
#       data:
#         - secretKey: config.yaml
#           remoteRef:
#             key: secrets/aikido/config
extraObjects: []

threatdetection:
  enabled: true
  image:
    pullPolicy: IfNotPresent
    registry: docker.io
    repository: falcosecurity/falco
    tag: "0.41.3"

  imagePullSecrets: [ ]
  nameOverride: ""
  fullnameOverride: ""
  namespaceOverride: ""
  podAnnotations: { }

  serviceAccount:
    imagePullSecrets: [ ]
    create: true
    annotations: { }
    name: ""

  rbac:
    create: true

  podLabels: { }
  podPriorityClassName:
  podSecurityContext: { }
  containerSecurityContext: { }

  resources:
    requests:
      cpu: 100m
      memory: 512Mi
    limits:
      cpu: 1000m
      memory: 1024Mi

  nodeSelector: { }
  affinity: { }

  tolerations:
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane

  healthChecks:
    livenessProbe:
      initialDelaySeconds: 60
      timeoutSeconds: 5
      periodSeconds: 15
    readinessProbe:
      initialDelaySeconds: 30
      timeoutSeconds: 5
      periodSeconds: 15

  tty: false

  controller:
    annotations: {}
    labels: {}
    daemonset:
      updateStrategy:
        type: RollingUpdate

  driver:
    enabled: true
    kind: auto
    kmod:
      bufSizePreset: 4
      dropFailedExit: false
    ebpf:
      path: "${HOME}/.falco/falco-bpf.o"
      hostNetwork: false
      leastPrivileged: false
      bufSizePreset: 4
      dropFailedExit: false
    modernEbpf:
      leastPrivileged: false
      bufSizePreset: 4
      dropFailedExit: false
      cpusForEachBuffer: 2

    loader:
      enabled: true
      initContainer:
        image:
          pullPolicy: IfNotPresent
          registry: docker.io
          repository: falcosecurity/falco-driver-loader
          tag: "0.41.3"

  collectors:
    enabled: true
    docker:
      enabled: false
    containerd:
      enabled: false
    crio:
      enabled: false

    containerEngine:
      enabled: true
      pluginRef: "ghcr.io/falcosecurity/plugins/plugin/container:0.3.6"
      labelMaxLen: 100
      withSize: false
      hooks: [ "create" ]
      engines:
        docker:
          enabled: true
          sockets: [ "/var/run/docker.sock" ]
        podman:
          enabled: true
          sockets: [ "/run/podman/podman.sock" ]
        containerd:
          enabled: true
          sockets: [ "/run/host-containerd/containerd.sock" ]
        cri:
          enabled: true
          sockets:
            [
              "/run/containerd/containerd.sock",
              "/run/crio/crio.sock",
              "/run/k3s/containerd/containerd.sock",
              "/run/host-containerd/containerd.sock",
            ]
        lxc:
          enabled: true
        libvirt_lxc:
          enabled: true
        bpm:
          enabled: true
  extra:
    env: [ ]
    args: [ ]
    initContainers: [ ]

  podHostname:

  falcoctl:
    image:
      pullPolicy: IfNotPresent
      registry: docker.io
      repository: falcosecurity/falcoctl
      tag: "0.11.2"
    artifact:
      install:
        enabled: true
        env: [ ]
        envFrom: [ ]
        args: [ "--log-format=json" ]
        resources: { }
        securityContext: { }
        mounts:
          volumeMounts: [ ]
      follow:
        enabled: false
        env: [ ]
        envFrom: [ ]
        args: [ "--log-format=json" ]
        resources: { }
        securityContext: { }
        mounts:
          volumeMounts: [ ]
    config:
      indexes:
        - name: falcosecurity
          url: https://falcosecurity.github.io/falcoctl/index.yaml
      artifact:
        allowedTypes:
          - plugin
        install:
          resolveDeps: true
          refs: [ ]
          rulesfilesDir: /rulesfiles
          pluginsDir: /plugins
        follow:
          refs: [ ]
          every: 6h
          falcoversions: http://localhost:8765/versions
          rulesfilesDir: /rulesfiles
          pluginsDir: /plugins

  falco:
    rules_files:
      - /etc/falco/falco_rules.yaml
      - /etc/falco/falco_rules.local.yaml
      - /etc/falco/rules.d
      - /etc/falco/aikido-rules.d
    rule_matching: first

    outputs_queue:
      capacity: 0

    load_plugins: [ ]
    plugins: []

    podHostname:

    config_files:
      - /etc/falco/config.d
    watch_config_files: true

    time_format_iso_8601: false

    priority: debug

    json_output: true
    json_include_output_property: true
    json_include_tags_property: true
    json_include_message_property: false
    json_include_output_fields_property: true

    buffered_outputs: false
    append_output:
      - suggested_output: true
    stdout_output:
      enabled: true
    syslog_output:
      enabled: true
    file_output:
      enabled: false
      keep_alive: false
      filename: ./events.txt
    http_output:
      enabled: true
      url: http://aikido-kubernetes-agent:8241/detection

    program_output:
      enabled: false
      keep_alive: false
      program: "jq '{text: .output}' | curl -d @- -X POST https://hooks.slack.com/services/XXX"

    webserver:
      enabled: true
      threadiness: 0
      listen_port: 8765
      k8s_healthz_endpoint: /healthz
      prometheus_metrics_enabled: false
      ssl_enabled: false
      ssl_certificate: /etc/falco/falco.pem

    log_stderr: true
    log_syslog: true
    log_level: info
    libs_logger:
      enabled: true
      severity: info
    output_timeout: 2000
    syscall_event_timeouts:
      max_consecutives: 1000

    syscall_event_drops:
      threshold: .1
      actions:
        - log
        - alert
      rate: .03333
      max_burst: 1
      simulate_drops: false

    base_syscalls:
      custom_set: [ ]
      repair: false

    falco_libs:
      thread_table_size: 262144

    container_engines:
      docker:
        enabled: false
      cri:
        enabled: false
        sockets:
          [
            "/run/containerd/containerd.sock",
            "/run/crio/crio.sock",
            "/run/k3s/containerd/containerd.sock",
          ]
        disable_async: false
      podman:
        enabled: false
      lxc:
        enabled: false
      libvirt_lxc:
        enabled: false
      bpm:
        enabled: false
